{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bc2b162c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, RandomSampler, SequentialSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4b23812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Title: Feminist Standpoint Theory: Women's Rol...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Title: Mitigating Gentrification: How Creative...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Title: Dreamers, Drugs, and Duties: How Racist...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Title: Bharti Airtel Business Methods\\n\\nIntro...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Title: Did Popper Solve the Problem of Inducti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            content  label\n",
       "0           0  Title: Feminist Standpoint Theory: Women's Rol...      1\n",
       "1           1  Title: Mitigating Gentrification: How Creative...      1\n",
       "2           2  Title: Dreamers, Drugs, and Duties: How Racist...      1\n",
       "3           3  Title: Bharti Airtel Business Methods\\n\\nIntro...      1\n",
       "4           4  Title: Did Popper Solve the Problem of Inducti...      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data from csv files\n",
    "\n",
    "df_gpt4 = pd.read_csv(\"/Users/maxschaffelder/Desktop/Thesis/Data/GPT4/data_GPT4_para1.csv\")\n",
    "df_human = pd.read_csv(\"/Users/maxschaffelder/Desktop/Thesis/Data/Human/data_human_para2.csv\")\n",
    "\n",
    "# Remove unnecessary columns\n",
    "df_gpt4 = df_gpt4.drop([\"prompt\"], axis=1)\n",
    "\n",
    "# Add labels for classification\n",
    "df_gpt4[\"label\"] = 1\n",
    "df_human[\"label\"] = 0\n",
    "\n",
    "# Combine human and GPT datasets into two datasets, one for gpt3.5 and one for gpt4\n",
    "df_4 = pd.concat([df_gpt4, df_human])\n",
    "\n",
    "df_4.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05b9fc13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Title: Feminist Standpoint Theory: Women's Rol...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Title: Mitigating Gentrification: How Creative...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Title: Dreamers, Drugs, and Duties: How Racist...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Title: Bharti Airtel Business Methods\\n\\nIntro...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Title: Did Popper Solve the Problem of Inducti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            content  label\n",
       "0           0  Title: Feminist Standpoint Theory: Women's Rol...      1\n",
       "1           1  Title: Mitigating Gentrification: How Creative...      1\n",
       "2           2  Title: Dreamers, Drugs, and Duties: How Racist...      1\n",
       "3           3  Title: Bharti Airtel Business Methods\\n\\nIntro...      1\n",
       "4           4  Title: Did Popper Solve the Problem of Inducti...      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffle datasets\n",
    "\n",
    "#df_4 = df_4.sample(frac=1, random_state=42)\n",
    "df_4 = df_4.reset_index(drop=True)\n",
    "\n",
    "df_4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d23a25e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "      <th>paragraph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Title: Feminist Standpoint Theory: Women's Rol...</td>\n",
       "      <td>1</td>\n",
       "      <td>Title: Feminist Standpoint Theory: Women's Rol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Title: Feminist Standpoint Theory: Women's Rol...</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Title: Feminist Standpoint Theory: Women's Rol...</td>\n",
       "      <td>1</td>\n",
       "      <td>Introduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Title: Feminist Standpoint Theory: Women's Rol...</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Title: Feminist Standpoint Theory: Women's Rol...</td>\n",
       "      <td>1</td>\n",
       "      <td>Feminist standpoint theory is an interdiscipli...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            content  label  \\\n",
       "0           0  Title: Feminist Standpoint Theory: Women's Rol...      1   \n",
       "1           0  Title: Feminist Standpoint Theory: Women's Rol...      1   \n",
       "2           0  Title: Feminist Standpoint Theory: Women's Rol...      1   \n",
       "3           0  Title: Feminist Standpoint Theory: Women's Rol...      1   \n",
       "4           0  Title: Feminist Standpoint Theory: Women's Rol...      1   \n",
       "\n",
       "                                           paragraph  \n",
       "0  Title: Feminist Standpoint Theory: Women's Rol...  \n",
       "1                                                     \n",
       "2                                       Introduction  \n",
       "3                                                     \n",
       "4  Feminist standpoint theory is an interdiscipli...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate data into paragraphs\n",
    "\n",
    "def separate_paragraphs(df, essay_column_name):\n",
    "    essays_by_paragraph = []\n",
    "\n",
    "     # Create a list to store the parsed paragraphs and essay indices\n",
    "    parsed_data = []\n",
    "\n",
    "    # Iterate over the essays in the specified column\n",
    "    for essay_index, essay in enumerate(df[essay_column_name]):\n",
    "        \n",
    "        # Separate the essay into paragraphs\n",
    "        paragraphs = essay.split('\\n')\n",
    "\n",
    "        # Create a list of tuples with the essay index and paragraph text\n",
    "        for paragraph in paragraphs:\n",
    "            parsed_data.append((essay_index, paragraph))\n",
    "\n",
    "    # Create a new DataFrame with the parsed paragraphs and essay indices\n",
    "    parsed_df = pd.DataFrame(parsed_data, columns=['essay_index', 'paragraph'])\n",
    "\n",
    "    # Merge the original DataFrame with the parsed DataFrame\n",
    "    merged_df = df.merge(parsed_df, left_index=True, right_on='essay_index')\n",
    "\n",
    "    # Drop the essay_index column as it is not needed anymore\n",
    "    merged_df.drop('essay_index', axis=1, inplace=True)\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "df_4 = separate_paragraphs(df_4, \"content\")\n",
    "df_4.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c38239f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize all data in the dataframe, by paragraph, making each entry a maximum of 512 tokens, \n",
    "# and adding padding if it's shorter\n",
    "\n",
    "def tokenize_df(df):\n",
    "    \n",
    "    # import tokenizer\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base') \n",
    "    \n",
    "    # initialize columns of new df\n",
    "    tokenized_df_columns = [\"tokenized text\", \"attention mask\", \"label\"]\n",
    "    \n",
    "    # initialize new df for tokenized data\n",
    "    tokenized_df = pd.DataFrame({col: [] for col in tokenized_df_columns})\n",
    "    \n",
    "    # temporary variable storing tokenized text\n",
    "    combined_paragraph_tokens = []\n",
    "    combined_paragraph_attention_masks = []\n",
    "    \n",
    "    # variable storing the current essay number in order to not combine different essays\n",
    "    current_essay_nr = df[\"Unnamed: 0\"][0] # initial value is first essay in list\n",
    "    \n",
    "    max_len = 512 # roberta model has a maximum input size of 512\n",
    "    \n",
    "    # Looping through original df (non-tokenized), getting paragraphs, labels, and essay number\n",
    "    for paragraph, label, essay_nr in zip(df[\"paragraph\"], df[\"label\"], df[\"Unnamed: 0\"]):\n",
    "\n",
    "        # encoding the current paragraph\n",
    "        paragraph_tokens = tokenizer.encode(paragraph) # tokens\n",
    "        paragraph_attention_mask = tokenizer(paragraph)[\"attention_mask\"] # attention mask\n",
    "        \n",
    "        # checking that 512 token length is not surpassed\n",
    "        if len(combined_paragraph_tokens) + len(paragraph_tokens) <= max_len and essay_nr == current_essay_nr:\n",
    "            #add new tokens\n",
    "            combined_paragraph_tokens = combined_paragraph_tokens + paragraph_tokens \n",
    "            # add new attention maskss\n",
    "            combined_paragraph_attention_masks = combined_paragraph_attention_masks + paragraph_attention_mask \n",
    "            \n",
    "        # else if it would be too long, add entry and start new one\n",
    "        elif len(combined_paragraph_tokens) + len(paragraph_tokens) > max_len:\n",
    "            \n",
    "            # add padding\n",
    "            padding_amount = max_len - len(combined_paragraph_tokens)\n",
    "            padding = [1 for i in range(padding_amount)]\n",
    "            padding_attention_mask = [0 for i in range(padding_amount)]\n",
    "            \n",
    "            combined_paragraph_tokens = combined_paragraph_tokens + padding\n",
    "            combined_paragraph_attention_masks = combined_paragraph_attention_masks + padding_attention_mask\n",
    "            \n",
    "            new_entry = {'tokenized text': combined_paragraph_tokens, \"attention mask\": combined_paragraph_attention_masks, 'label': label}\n",
    "            tokenized_df.loc[len(tokenized_df)] = new_entry\n",
    "            combined_paragraph_tokens = []\n",
    "            combined_paragraph_attention_masks = []\n",
    "            \n",
    "        # else if a new essay has started\n",
    "        elif essay_nr != current_essay_nr:\n",
    "            \n",
    "            # add padding\n",
    "            padding_amount = max_len - len(combined_paragraph_tokens)\n",
    "            padding = [1 for i in range(padding_amount)]\n",
    "            padding_attention_mask = [0 for i in range(padding_amount)]\n",
    "            combined_paragraph_tokens = combined_paragraph_tokens + padding\n",
    "            combined_paragraph_attention_masks = combined_paragraph_attention_masks + padding_attention_mask\n",
    "            \n",
    "            new_entry = {'tokenized text': combined_paragraph_tokens, \"attention mask\": combined_paragraph_attention_masks, 'label': label}\n",
    "            tokenized_df.loc[len(tokenized_df)] = new_entry\n",
    "            combined_paragraph_tokens = []\n",
    "            combined_paragraph_attention_masks = []\n",
    "            current_essay_nr = essay_nr\n",
    "           \n",
    "    return tokenized_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24586942",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maxschaffelder/opt/anaconda3/lib/python3.9/site-packages/pandas/core/dtypes/cast.py:883: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  element = np.asarray(element)\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (520 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenized text</th>\n",
       "      <th>attention mask</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0, 46525, 35, 43588, 13371, 2300, 26305, 35, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0, 2, 0, 597, 20554, 661, 14922, 6680, 34, 11...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0, 2, 0, 9167, 1517, 2577, 17799, 14922, 6680...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0, 2, 0, 46576, 2, 0, 2, 0, 534, 1342, 34136,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0, 2, 0, 246, 4, 31392, 5206, 11176, 3109, 44...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      tokenized text  \\\n",
       "0  [0, 46525, 35, 43588, 13371, 2300, 26305, 35, ...   \n",
       "1  [0, 2, 0, 597, 20554, 661, 14922, 6680, 34, 11...   \n",
       "2  [0, 2, 0, 9167, 1517, 2577, 17799, 14922, 6680...   \n",
       "3  [0, 2, 0, 46576, 2, 0, 2, 0, 534, 1342, 34136,...   \n",
       "4  [0, 2, 0, 246, 4, 31392, 5206, 11176, 3109, 44...   \n",
       "\n",
       "                                      attention mask  label  \n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      1  \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      1  \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      1  \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      1  \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_df4 = tokenize_df(df_4)\n",
    "tokenized_df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83a0c74c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "596"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove rows where the list only contains 1s or starts with [0, 2] and contains only 1s afterwards\n",
    "tokenized_df4 = tokenized_df4.loc[~tokenized_df4['tokenized text'].apply(lambda x: all(e == 1 for e in x) or (len(x) >= 2 and x[:2] == [0, 2] and all(e == 1 for e in x[2:]))), :]\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "len(tokenized_df4[\"tokenized text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03727db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "train_data, test_data = train_test_split(tokenized_df4, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the test set further into validation and test sets\n",
    "val_data, test_data = train_test_split(test_data, test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13ec3bfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "476"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data[\"tokenized text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b93dcded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "596\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenized text</th>\n",
       "      <th>attention mask</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0, 46525, 35, 43588, 13371, 2300, 26305, 35, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0, 2, 0, 597, 20554, 661, 14922, 6680, 34, 11...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0, 2, 0, 9167, 1517, 2577, 17799, 14922, 6680...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0, 2, 0, 46576, 2, 0, 2, 0, 534, 1342, 34136,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0, 2, 0, 246, 4, 31392, 5206, 11176, 3109, 44...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      tokenized text  \\\n",
       "0  [0, 46525, 35, 43588, 13371, 2300, 26305, 35, ...   \n",
       "1  [0, 2, 0, 597, 20554, 661, 14922, 6680, 34, 11...   \n",
       "2  [0, 2, 0, 9167, 1517, 2577, 17799, 14922, 6680...   \n",
       "3  [0, 2, 0, 46576, 2, 0, 2, 0, 534, 1342, 34136,...   \n",
       "4  [0, 2, 0, 246, 4, 31392, 5206, 11176, 3109, 44...   \n",
       "\n",
       "                                      attention mask  label  \n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      1  \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      1  \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      1  \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      1  \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      1  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths = []\n",
    "\n",
    "print(len(tokenized_df4))\n",
    "#for i in tokenized_df[\"tokenized_text\"]:\n",
    " #   print(i)\n",
    "    \n",
    "#print(tokenized_df[\"tokenized text\"][60])    \n",
    "#print(tokenized_df[\"attention mask\"][60])\n",
    "tokenized_df4.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d7a120bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "476"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6d1601de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Create model (ADD TO OTHER CODE)\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82a308f4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Convert pd dataframes into pytorch tensors\n",
    "\n",
    "train_inputs = torch.tensor(train_data[\"tokenized text\"].tolist())\n",
    "train_masks = torch.tensor(train_data[\"attention mask\"].tolist())\n",
    "train_labels = torch.tensor(train_data[\"label\"].tolist())\n",
    "\n",
    "\n",
    "validation_inputs = torch.tensor(val_data[\"tokenized text\"].tolist())\n",
    "validation_masks = torch.tensor(val_data[\"attention mask\"].tolist())\n",
    "validation_labels = torch.tensor(val_data[\"label\"].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "38634de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloader\n",
    "\n",
    "batch_size = 16 # try different values\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209d8230",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "print_every = 100\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        \n",
    "        # Unpack the batch into input tensors, attention masks, and labels\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "\n",
    "        # Clear any previously calculated gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Perform a forward pass through the model and compute the loss\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Perform a backward pass to compute gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the gradients to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update the model's weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "    model.eval()\n",
    "    eval_loss = 0.0\n",
    "    for batch in validation_dataloader:\n",
    "        # Unpack the batch into input tensors, attention masks, and labels\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "\n",
    "        # Disable gradient calculations, as they are not needed for evaluation\n",
    "        with torch.no_grad():\n",
    "            # Perform a forward pass through the model and compute the loss\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            # Accumulate the evaluation loss\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "    # Compute and print the average validation loss\n",
    "    avg_eval_loss = eval_loss / len(validation_dataloader)\n",
    "    print(f\"Epoch {epoch+1}, Validation Loss: {avg_eval_loss}\")\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85516c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3b34387e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s></s><s>However, Kane suggests that even if our choices are influenced by neural processes and genetic factors, they are not causally determined by them. He argues that individuals can still have ultimate responsibility for their choices if their choices are indeterminate and self-forming.</s><s></s><s>Human Sciences</s><s></s><s>The human sciences, such as psychology and sociology, also raise questions about the compatibility of libertarianism with what we know about human beings. These sciences suggest that our choices and actions are influenced by a variety of social, cultural, and environmental factors.</s><s></s><s>However, Kane argues that even if our choices are influenced by external factors, they are not causally determined by them. He suggests that individuals have the power to reflect on their values and choose to act in accordance with them, which allows them to make choices that are not causally determined by external factors.</s><s></s><s>Conclusion</s><s></s><s>In conclusion, the concept of free will has been debated for centuries, and the libertarian view of free will has been challenged by determinism. Additionally, the requirement of ultimate responsibility seems to imply the need for obscure or mysterious forms of agency or causation. However, Robert Kane has attempted to reconcile libertarianism with ultimate responsibility without appealing to obscure or mysterious forms of agency or causation.</s><s></s><s>Kane argues that indeterminacy and self-forming actions are necessary for individuals to have ultimate responsibility for their choices and actions. He suggests that individuals have the power to make choices that are not causally determined by external factors, which allows them to have ultimate responsibility for their actions.</s><s></s><s>While Kaneâ€™s attempt to reconcile libertarianism with ultimate responsibility raises questions about the compatibility of this view with what we know about human beings in the modern physical, biological, and human sciences, his arguments suggest that individuals have the power to make choices that are not causally determined by external factors.</s><s></s><s>Overall, the question of whether a libertarian view of free will requiring ultimate responsibility can be made intelligible without appealing to obscure or mysterious forms of agency or causation remains a topic of debate in philosophy.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = \"This is an example sentence.\"\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base') \n",
    "\n",
    "# Using encode()\n",
    "token_ids = tokenizer.encode(input_text)\n",
    "\n",
    "# Using __call__()\n",
    "encoded_input = tokenizer(input_text)\n",
    "\n",
    "tokenizer.decode([0, 2, 0, 10462, 6, 8281, 3649, 14, 190, 114, 84, 5717, 32, 11359, 30, 26739, 5588, 8, 9186, 2433, 6, 51, 32, 45, 37771, 2368, 3030, 30, 106, 4, 91, 10648, 14, 2172, 64, 202, 33, 7017, 2640, 13, 49, 5717, 114, 49, 5717, 32, 9473, 39938, 877, 8, 1403, 12, 11847, 4, 2, 0, 2, 0, 33837, 8841, 2, 0, 2, 0, 133, 1050, 17874, 6, 215, 25, 16797, 8, 35638, 6, 67, 1693, 1142, 59, 5, 29988, 9, 36471, 1809, 19, 99, 52, 216, 59, 1050, 14766, 4, 1216, 17874, 3608, 14, 84, 5717, 8, 2163, 32, 11359, 30, 10, 3143, 9, 592, 6, 4106, 6, 8, 3039, 2433, 4, 2, 0, 2, 0, 10462, 6, 8281, 10648, 14, 190, 114, 84, 5717, 32, 11359, 30, 6731, 2433, 6, 51, 32, 45, 37771, 2368, 3030, 30, 106, 4, 91, 3649, 14, 2172, 33, 5, 476, 7, 4227, 15, 49, 3266, 8, 2807, 7, 1760, 11, 10753, 19, 106, 6, 61, 2386, 106, 7, 146, 5717, 14, 32, 45, 37771, 2368, 3030, 30, 6731, 2433, 4, 2, 0, 2, 0, 48984, 2, 0, 2, 0, 1121, 6427, 6, 5, 4286, 9, 481, 40, 34, 57, 19639, 13, 11505, 6, 8, 5, 36471, 1217, 9, 481, 40, 34, 57, 6835, 30, 26948, 1809, 4, 6903, 6, 5, 7404, 9, 7017, 2640, 1302, 7, 25696, 5, 240, 13, 23732, 50, 12754, 4620, 9, 1218, 50, 45832, 4, 635, 6, 1738, 8281, 34, 3751, 7, 27389, 36471, 1809, 19, 7017, 2640, 396, 9364, 7, 23732, 50, 12754, 4620, 9, 1218, 50, 45832, 4, 2, 0, 2, 0, 530, 1728, 10648, 14, 9473, 39938, 5073, 8, 1403, 12, 11847, 2163, 32, 2139, 13, 2172, 7, 33, 7017, 2640, 13, 49, 5717, 8, 2163, 4, 91, 3649, 14, 2172, 33, 5, 476, 7, 146, 5717, 14, 32, 45, 37771, 2368, 3030, 30, 6731, 2433, 6, 61, 2386, 106, 7, 33, 7017, 2640, 13, 49, 2163, 4, 2, 0, 2, 0, 5771, 8281, 17, 27, 29, 2120, 7, 27389, 36471, 1809, 19, 7017, 2640, 7700, 1142, 59, 5, 29988, 9, 42, 1217, 19, 99, 52, 216, 59, 1050, 14766, 11, 5, 2297, 2166, 6, 12243, 6, 8, 1050, 17874, 6, 39, 7576, 3608, 14, 2172, 33, 5, 476, 7, 146, 5717, 14, 32, 45, 37771, 2368, 3030, 30, 6731, 2433, 4, 2, 0, 2, 0, 28965, 6, 5, 864, 9, 549, 10, 36471, 1217, 9, 481, 40, 7980, 7017, 2640, 64, 28, 156, 37806, 4748, 396, 9364, 7, 23732, 50, 12754, 4620, 9, 1218, 50, 45832, 1189, 10, 5674, 9, 2625, 11, 10561, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
    "\n",
    "#encoded_input[\"attention_mask\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a8c7a56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 713, 16, 41, 1246, 4, 2, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "text = \"This is an example.\"\n",
    "\n",
    "# Tokenize the text and add padding\n",
    "encoded = tokenizer.encode(text, padding='max_length', max_length=10, truncation=True)\n",
    "\n",
    "print(encoded)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
