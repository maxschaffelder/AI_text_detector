{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bc2b162c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4b23812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Introduction \\n\\nFeminist Standpoint Theory is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Mitigating gentrification: How creative design...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Introduction\\n\\nDrug use and abuse are signifi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Introduction\\n\\nBharti Airtel is one of the le...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Introduction\\n\\nThe problem of induction is a ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            content  label\n",
       "0           0  Introduction \\n\\nFeminist Standpoint Theory is...      1\n",
       "1           1  Mitigating gentrification: How creative design...      1\n",
       "2           2  Introduction\\n\\nDrug use and abuse are signifi...      1\n",
       "3           3  Introduction\\n\\nBharti Airtel is one of the le...      1\n",
       "4           4  Introduction\\n\\nThe problem of induction is a ...      1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data from csv files\n",
    "df_gpt3 = pd.read_csv(\"/Users/maxschaffelder/Desktop/Thesis/Data/GPT3/data_GPT3_para3.csv\")\n",
    "df_human = pd.read_csv(\"/Users/maxschaffelder/Desktop/Thesis/Data/Human/data_human_para2.csv\")\n",
    "\n",
    "# Remove unnecessary columns\n",
    "df_gpt3 = df_gpt3.drop([\"prompt\"], axis=1)\n",
    "\n",
    "# Add labels for classification\n",
    "df_gpt3[\"label\"] = 1\n",
    "df_human[\"label\"] = 0\n",
    "\n",
    "# Combine human and GPT datasets into two datasets, one for gpt3.5 and one for gpt4\n",
    "df_3 = pd.concat([df_gpt3, df_human])\n",
    "\n",
    "df_3.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "05b9fc13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Introduction \\n\\nFeminist Standpoint Theory is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Mitigating gentrification: How creative design...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Introduction\\n\\nDrug use and abuse are signifi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Introduction\\n\\nBharti Airtel is one of the le...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Introduction\\n\\nThe problem of induction is a ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            content  label\n",
       "0           0  Introduction \\n\\nFeminist Standpoint Theory is...      1\n",
       "1           1  Mitigating gentrification: How creative design...      1\n",
       "2           2  Introduction\\n\\nDrug use and abuse are signifi...      1\n",
       "3           3  Introduction\\n\\nBharti Airtel is one of the le...      1\n",
       "4           4  Introduction\\n\\nThe problem of induction is a ...      1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffle datasets\n",
    "#df_3 = df_3.sample(frac=1, random_state=42)\n",
    "df_3 = df_3.reset_index(drop=True)\n",
    "\n",
    "df_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0d23a25e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "      <th>paragraph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Introduction \\n\\nFeminist Standpoint Theory is...</td>\n",
       "      <td>1</td>\n",
       "      <td>Introduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Introduction \\n\\nFeminist Standpoint Theory is...</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Introduction \\n\\nFeminist Standpoint Theory is...</td>\n",
       "      <td>1</td>\n",
       "      <td>Feminist Standpoint Theory is a school of thou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Introduction \\n\\nFeminist Standpoint Theory is...</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Introduction \\n\\nFeminist Standpoint Theory is...</td>\n",
       "      <td>1</td>\n",
       "      <td>Women's Historical Marginalization in Science</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            content  label  \\\n",
       "0           0  Introduction \\n\\nFeminist Standpoint Theory is...      1   \n",
       "1           0  Introduction \\n\\nFeminist Standpoint Theory is...      1   \n",
       "2           0  Introduction \\n\\nFeminist Standpoint Theory is...      1   \n",
       "3           0  Introduction \\n\\nFeminist Standpoint Theory is...      1   \n",
       "4           0  Introduction \\n\\nFeminist Standpoint Theory is...      1   \n",
       "\n",
       "                                           paragraph  \n",
       "0                                      Introduction   \n",
       "1                                                     \n",
       "2  Feminist Standpoint Theory is a school of thou...  \n",
       "3                                                     \n",
       "4     Women's Historical Marginalization in Science   "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate data into paragraphs\n",
    "\n",
    "def separate_paragraphs(df, essay_column_name):\n",
    "    essays_by_paragraph = []\n",
    "\n",
    "     # Create a list to store the parsed paragraphs and essay indices\n",
    "    parsed_data = []\n",
    "\n",
    "    # Iterate over the essays in the specified column\n",
    "    for essay_index, essay in enumerate(df[essay_column_name]):\n",
    "        \n",
    "        # Separate the essay into paragraphs\n",
    "        paragraphs = essay.split('\\n')\n",
    "\n",
    "        # Create a list of tuples with the essay index and paragraph text\n",
    "        for paragraph in paragraphs:\n",
    "            parsed_data.append((essay_index, paragraph))\n",
    "\n",
    "    # Create a new DataFrame with the parsed paragraphs and essay indices\n",
    "    parsed_df = pd.DataFrame(parsed_data, columns=['essay_index', 'paragraph'])\n",
    "\n",
    "    # Merge the original DataFrame with the parsed DataFrame\n",
    "    merged_df = df.merge(parsed_df, left_index=True, right_on='essay_index')\n",
    "\n",
    "    # Drop the essay_index column as it is not needed anymore\n",
    "    merged_df.drop('essay_index', axis=1, inplace=True)\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "df_3 = separate_paragraphs(df_3, \"content\")\n",
    "df_3.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c38239f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize all data in the dataframe, by paragraph, making each entry a maximum of 512 tokens, \n",
    "# and adding padding if it's shorter\n",
    "\n",
    "def tokenize_df(df):\n",
    "    \n",
    "    # import tokenizer\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base') \n",
    "    \n",
    "    # initialize columns of new df\n",
    "    tokenized_df_columns = [\"tokenized text\", \"attention mask\", \"label\"]\n",
    "    \n",
    "    # initialize new df for tokenized data\n",
    "    tokenized_df = pd.DataFrame({col: [] for col in tokenized_df_columns})\n",
    "    \n",
    "    # temporary variable storing tokenized text\n",
    "    combined_paragraph_tokens = []\n",
    "    combined_paragraph_attention_masks = []\n",
    "    \n",
    "    # variable storing the current essay number in order to not combine different essays\n",
    "    current_essay_nr = df[\"Unnamed: 0\"][0] # initial value is first essay in list\n",
    "    \n",
    "    max_len = 512 # roberta model has a maximum input size of 512\n",
    "    \n",
    "    # Looping through original df (non-tokenized), getting paragraphs, labels, and essay number\n",
    "    for paragraph, label, essay_nr in zip(df[\"paragraph\"], df[\"label\"], df[\"Unnamed: 0\"]):\n",
    "\n",
    "        # encoding the current paragraph\n",
    "        paragraph_tokens = tokenizer.encode(paragraph) # tokens\n",
    "        paragraph_attention_mask = tokenizer(paragraph)[\"attention_mask\"] # attention mask\n",
    "        \n",
    "        # checking that 512 token length is not surpassed\n",
    "        if len(combined_paragraph_tokens) + len(paragraph_tokens) <= max_len and essay_nr == current_essay_nr:\n",
    "            #add new tokens\n",
    "            combined_paragraph_tokens = combined_paragraph_tokens + paragraph_tokens \n",
    "            # add new attention maskss\n",
    "            combined_paragraph_attention_masks = combined_paragraph_attention_masks + paragraph_attention_mask \n",
    "            \n",
    "        # else if it would be too long, add entry and start new one\n",
    "        elif len(combined_paragraph_tokens) + len(paragraph_tokens) > max_len:\n",
    "            \n",
    "            # add padding\n",
    "            padding_amount = max_len - len(combined_paragraph_tokens)\n",
    "            padding = [1 for i in range(padding_amount)]\n",
    "            padding_attention_mask = [0 for i in range(padding_amount)]\n",
    "            \n",
    "            combined_paragraph_tokens = combined_paragraph_tokens + padding\n",
    "            combined_paragraph_attention_masks = combined_paragraph_attention_masks + padding_attention_mask\n",
    "            \n",
    "            new_entry = {'tokenized text': combined_paragraph_tokens, \"attention mask\": combined_paragraph_attention_masks, 'label': label}\n",
    "            tokenized_df.loc[len(tokenized_df)] = new_entry\n",
    "            combined_paragraph_tokens = []\n",
    "            combined_paragraph_attention_masks = []\n",
    "            \n",
    "        # else if a new essay has started\n",
    "        elif essay_nr != current_essay_nr:\n",
    "            \n",
    "            # add padding\n",
    "            padding_amount = max_len - len(combined_paragraph_tokens)\n",
    "            padding = [1 for i in range(padding_amount)]\n",
    "            padding_attention_mask = [0 for i in range(padding_amount)]\n",
    "            combined_paragraph_tokens = combined_paragraph_tokens + padding\n",
    "            combined_paragraph_attention_masks = combined_paragraph_attention_masks + padding_attention_mask\n",
    "            \n",
    "            new_entry = {'tokenized text': combined_paragraph_tokens, \"attention mask\": combined_paragraph_attention_masks, 'label': label}\n",
    "            tokenized_df.loc[len(tokenized_df)] = new_entry\n",
    "            combined_paragraph_tokens = []\n",
    "            combined_paragraph_attention_masks = []\n",
    "            current_essay_nr = essay_nr\n",
    "           \n",
    "    return tokenized_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "24586942",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maxschaffelder/opt/anaconda3/lib/python3.9/site-packages/pandas/core/dtypes/cast.py:883: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  element = np.asarray(element)\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (520 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenized text</th>\n",
       "      <th>attention mask</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0, 46576, 1437, 2, 0, 2, 0, 597, 20554, 661, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0, 2, 0, 597, 20554, 661, 13371, 2300, 26305,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0, 2, 0, 21518, 1246, 16, 5, 882, 9, 3039, 24...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0, 2, 0, 46576, 2, 0, 2, 0, 534, 1342, 34136,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0, 2, 0, 19247, 5938, 2, 0, 2, 0, 19247, 5938...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      tokenized text  \\\n",
       "0  [0, 46576, 1437, 2, 0, 2, 0, 597, 20554, 661, ...   \n",
       "1  [0, 2, 0, 597, 20554, 661, 13371, 2300, 26305,...   \n",
       "2  [0, 2, 0, 21518, 1246, 16, 5, 882, 9, 3039, 24...   \n",
       "3  [0, 2, 0, 46576, 2, 0, 2, 0, 534, 1342, 34136,...   \n",
       "4  [0, 2, 0, 19247, 5938, 2, 0, 2, 0, 19247, 5938...   \n",
       "\n",
       "                                      attention mask  label  \n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      1  \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      1  \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      1  \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      1  \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      1  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_df3 = tokenize_df(df_3)\n",
    "tokenized_df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "83a0c74c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "554"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove rows where the list only contains 1s or starts with [0, 2] and contains only 1s afterwards\n",
    "tokenized_df3 = tokenized_df3.loc[~tokenized_df3['tokenized text'].apply(lambda x: all(e == 1 for e in x) or (len(x) >= 2 and x[:2] == [0, 2] and all(e == 1 for e in x[2:]))), :]\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "len(tokenized_df3[\"tokenized text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "03727db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "train_data, test_data = train_test_split(tokenized_df3, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the test set further into validation and test sets\n",
    "val_data, test_data = train_test_split(test_data, test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "13ec3bfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "443"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data[\"tokenized text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b93dcded",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = []\n",
    "\n",
    "#print(len(tokenized_df4))\n",
    "#for i in tokenized_df[\"tokenized_text\"]:\n",
    " #   print(i)\n",
    "    \n",
    "#print(tokenized_df[\"tokenized text\"][60])    \n",
    "#print(tokenized_df[\"attention mask\"][60])\n",
    "#tokenized_df4.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8d0149af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727236d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert \n",
    "\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "validation_labels = torch.tensor(validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61fe197",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3b34387e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s></s><s>However, Kane suggests that even if our choices are influenced by neural processes and genetic factors, they are not causally determined by them. He argues that individuals can still have ultimate responsibility for their choices if their choices are indeterminate and self-forming.</s><s></s><s>Human Sciences</s><s></s><s>The human sciences, such as psychology and sociology, also raise questions about the compatibility of libertarianism with what we know about human beings. These sciences suggest that our choices and actions are influenced by a variety of social, cultural, and environmental factors.</s><s></s><s>However, Kane argues that even if our choices are influenced by external factors, they are not causally determined by them. He suggests that individuals have the power to reflect on their values and choose to act in accordance with them, which allows them to make choices that are not causally determined by external factors.</s><s></s><s>Conclusion</s><s></s><s>In conclusion, the concept of free will has been debated for centuries, and the libertarian view of free will has been challenged by determinism. Additionally, the requirement of ultimate responsibility seems to imply the need for obscure or mysterious forms of agency or causation. However, Robert Kane has attempted to reconcile libertarianism with ultimate responsibility without appealing to obscure or mysterious forms of agency or causation.</s><s></s><s>Kane argues that indeterminacy and self-forming actions are necessary for individuals to have ultimate responsibility for their choices and actions. He suggests that individuals have the power to make choices that are not causally determined by external factors, which allows them to have ultimate responsibility for their actions.</s><s></s><s>While Kaneâ€™s attempt to reconcile libertarianism with ultimate responsibility raises questions about the compatibility of this view with what we know about human beings in the modern physical, biological, and human sciences, his arguments suggest that individuals have the power to make choices that are not causally determined by external factors.</s><s></s><s>Overall, the question of whether a libertarian view of free will requiring ultimate responsibility can be made intelligible without appealing to obscure or mysterious forms of agency or causation remains a topic of debate in philosophy.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = \"This is an example sentence.\"\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base') \n",
    "\n",
    "# Using encode()\n",
    "token_ids = tokenizer.encode(input_text)\n",
    "\n",
    "# Using __call__()\n",
    "encoded_input = tokenizer(input_text)\n",
    "\n",
    "tokenizer.decode([0, 2, 0, 10462, 6, 8281, 3649, 14, 190, 114, 84, 5717, 32, 11359, 30, 26739, 5588, 8, 9186, 2433, 6, 51, 32, 45, 37771, 2368, 3030, 30, 106, 4, 91, 10648, 14, 2172, 64, 202, 33, 7017, 2640, 13, 49, 5717, 114, 49, 5717, 32, 9473, 39938, 877, 8, 1403, 12, 11847, 4, 2, 0, 2, 0, 33837, 8841, 2, 0, 2, 0, 133, 1050, 17874, 6, 215, 25, 16797, 8, 35638, 6, 67, 1693, 1142, 59, 5, 29988, 9, 36471, 1809, 19, 99, 52, 216, 59, 1050, 14766, 4, 1216, 17874, 3608, 14, 84, 5717, 8, 2163, 32, 11359, 30, 10, 3143, 9, 592, 6, 4106, 6, 8, 3039, 2433, 4, 2, 0, 2, 0, 10462, 6, 8281, 10648, 14, 190, 114, 84, 5717, 32, 11359, 30, 6731, 2433, 6, 51, 32, 45, 37771, 2368, 3030, 30, 106, 4, 91, 3649, 14, 2172, 33, 5, 476, 7, 4227, 15, 49, 3266, 8, 2807, 7, 1760, 11, 10753, 19, 106, 6, 61, 2386, 106, 7, 146, 5717, 14, 32, 45, 37771, 2368, 3030, 30, 6731, 2433, 4, 2, 0, 2, 0, 48984, 2, 0, 2, 0, 1121, 6427, 6, 5, 4286, 9, 481, 40, 34, 57, 19639, 13, 11505, 6, 8, 5, 36471, 1217, 9, 481, 40, 34, 57, 6835, 30, 26948, 1809, 4, 6903, 6, 5, 7404, 9, 7017, 2640, 1302, 7, 25696, 5, 240, 13, 23732, 50, 12754, 4620, 9, 1218, 50, 45832, 4, 635, 6, 1738, 8281, 34, 3751, 7, 27389, 36471, 1809, 19, 7017, 2640, 396, 9364, 7, 23732, 50, 12754, 4620, 9, 1218, 50, 45832, 4, 2, 0, 2, 0, 530, 1728, 10648, 14, 9473, 39938, 5073, 8, 1403, 12, 11847, 2163, 32, 2139, 13, 2172, 7, 33, 7017, 2640, 13, 49, 5717, 8, 2163, 4, 91, 3649, 14, 2172, 33, 5, 476, 7, 146, 5717, 14, 32, 45, 37771, 2368, 3030, 30, 6731, 2433, 6, 61, 2386, 106, 7, 33, 7017, 2640, 13, 49, 2163, 4, 2, 0, 2, 0, 5771, 8281, 17, 27, 29, 2120, 7, 27389, 36471, 1809, 19, 7017, 2640, 7700, 1142, 59, 5, 29988, 9, 42, 1217, 19, 99, 52, 216, 59, 1050, 14766, 11, 5, 2297, 2166, 6, 12243, 6, 8, 1050, 17874, 6, 39, 7576, 3608, 14, 2172, 33, 5, 476, 7, 146, 5717, 14, 32, 45, 37771, 2368, 3030, 30, 6731, 2433, 4, 2, 0, 2, 0, 28965, 6, 5, 864, 9, 549, 10, 36471, 1217, 9, 481, 40, 7980, 7017, 2640, 64, 28, 156, 37806, 4748, 396, 9364, 7, 23732, 50, 12754, 4620, 9, 1218, 50, 45832, 1189, 10, 5674, 9, 2625, 11, 10561, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
    "\n",
    "#encoded_input[\"attention_mask\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a8c7a56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 713, 16, 41, 1246, 4, 2, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "text = \"This is an example.\"\n",
    "\n",
    "# Tokenize the text and add padding\n",
    "encoded = tokenizer.encode(text, padding='max_length', max_length=10, truncation=True)\n",
    "\n",
    "print(encoded)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
